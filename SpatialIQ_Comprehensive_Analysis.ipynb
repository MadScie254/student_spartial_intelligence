{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30b0da91",
   "metadata": {},
   "source": [
    "# SpatialIQ: Predicting Students' Spatial Intelligence through Machine Learning\n",
    "\n",
    "## A Comprehensive AI-in-Education Research Study\n",
    "\n",
    "**Author:** AI Research Developer  \n",
    "**Date:** November 2025  \n",
    "**Dataset Citation:** DOI: 10.21227/5qxw-bw66  \n",
    "**Purpose:** Interpretable prediction and analysis of spatial intelligence in high school students using behavioral, academic, and demographic features\n",
    "\n",
    "---\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "This notebook presents a rigorous machine learning pipeline designed to predict students' **Spatial Intelligence** levels (ranging from Very Low to Very High) using a rich dataset of 40 behavioral, academic, and demographic features collected from high school students. Spatial intelligence—the ability to visualize, manipulate, and reason about spatial relationships—is a critical component of cognitive development and academic achievement, particularly in STEM disciplines.\n",
    "\n",
    "### Research Questions:\n",
    "1. **What behavioral and academic patterns predict spatial intelligence?**\n",
    "2. **How do demographic factors (gender, parental education, environment) influence spatial reasoning ability?**\n",
    "3. **Can we build interpretable models that identify actionable insights for educators?**\n",
    "4. **What ethical considerations arise from AI-driven student cognitive profiling?**\n",
    "\n",
    "### Methodology Overview:\n",
    "- **Phase 1:** Comprehensive data exploration and statistical profiling\n",
    "- **Phase 2:** Intelligent feature engineering with domain knowledge\n",
    "- **Phase 3:** Competitive modeling with Logistic Regression, Random Forest, XGBoost, and Neural Networks\n",
    "- **Phase 4:** Deep model interpretation using SHAP explainability methods\n",
    "- **Phase 5:** Actionable insights and ethical considerations\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset Overview\n",
    "\n",
    "The dataset comprises 398 high school students with 40 features across five categories:\n",
    "- **Demographics:** Age, Gender, Class size, Environment (Urban/Suburban/Rural)\n",
    "- **Socioeconomic:** Family size, Parental occupation, Parental education, Income level\n",
    "- **Academic:** Major, GPA, Study time, Extra classes, Teacher assessment\n",
    "- **Behavioral:** Internet usage, TV watching, Pattern recognition, Geographic familiarity\n",
    "- **Gaming Preferences:** Action, Adventure, Strategy, Sport, Simulation, Role-playing, Puzzle games\n",
    "- **Learning Modes:** Visual vs. Auditory, Map usage, Diagram usage, Experience with GIS\n",
    "\n",
    "**Target Variable:** Spatial Intelligence (Categorical: VL, L, M, H, VH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d1d9e5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 1: Import Required Libraries and Dataset Loading\n",
    "\n",
    "This foundational section initializes all necessary libraries for data manipulation, visualization, machine learning, and model interpretation. We leverage industry-standard tools including pandas for data handling, scikit-learn for preprocessing and modeling, XGBoost for gradient boosting, TensorFlow for neural networks, and SHAP for model explainability. This comprehensive toolkit enables both rigorous statistical analysis and production-grade machine learning workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252c3951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== LIBRARY IMPORTS ====================\n",
    "# Core Data Science and Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Data Preprocessing and Encoding\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Model Selection, Training, and Evaluation\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Metrics and Evaluation\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score,\n",
    "    roc_curve, auc, matthews_corrcoef\n",
    ")\n",
    "\n",
    "# Statistical Analysis\n",
    "from scipy.stats import chi2_contingency, mutual_info_classif, spearmanr, pearsonr\n",
    "from scipy.stats import kurtosis, skew\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Feature Importance and SHAP Interpretability\n",
    "import shap\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Warnings and Configuration\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration for visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"✓ All libraries successfully imported\")\n",
    "print(f\"✓ NumPy version: {np.__version__}\")\n",
    "print(f\"✓ Pandas version: {pd.__version__}\")\n",
    "print(f\"✓ Scikit-learn version: {pd.__version__.split('.')[0]}\")\n",
    "print(f\"✓ SHAP library initialized for model interpretation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efe7f52",
   "metadata": {},
   "source": [
    "## 1.1: Load and Inspect the Dataset\n",
    "\n",
    "In this subsection, we load the Dataset.csv file and perform initial exploratory inspection to understand the data structure, dimensions, data types, and preliminary statistical characteristics. This foundational step is critical for identifying potential data quality issues, missing values, and the nature of features we'll be working with throughout the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f676aea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from CSV file\n",
    "data_path = Path(\"data/Dataset.csv\")\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASET LOADING AND INITIAL INSPECTION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n✓ Dataset successfully loaded from: {data_path}\")\n",
    "print(f\"\\nDataset Shape: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "print(f\"Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Display column names and data types\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COLUMN INFORMATION AND DATA TYPES\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nColumn Names and Data Types:\")\n",
    "print(df.info())\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FIRST 10 ROWS OF DATA\")\n",
    "print(\"=\" * 80)\n",
    "display(df.head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "display(df.describe().transpose())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MISSING VALUES ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_counts.index,\n",
    "    'Missing_Count': missing_counts.values,\n",
    "    'Missing_Percentage': missing_percentage.values\n",
    "})\n",
    "missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    print(f\"\\nColumns with missing values ({len(missing_df)} found):\")\n",
    "    display(missing_df)\n",
    "else:\n",
    "    print(\"\\n✓ No missing values detected in the dataset!\")\n",
    "\n",
    "# Identify target variable\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TARGET VARIABLE IDENTIFICATION\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nDataset column names:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")\n",
    "\n",
    "# The last column is typically the target (Spatial Intelligence level)\n",
    "target_col = df.columns[-1]\n",
    "print(f\"\\n✓ Identified target variable: '{target_col}'\")\n",
    "print(f\"Unique values in target: {df[target_col].unique()}\")\n",
    "print(f\"Target value counts:\\n{df[target_col].value_counts().sort_index()}\")\n",
    "\n",
    "# Create a reference dictionary for later use\n",
    "target_mapping = {val: idx for idx, val in enumerate(sorted(df[target_col].unique()))}\n",
    "print(f\"\\nTarget ordinal mapping: {target_mapping}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404e2f51",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 2: Exploratory Data Analysis and Statistical Profiling\n",
    "\n",
    "## 2.1: Distribution Analysis of Spatial Intelligence\n",
    "\n",
    "Spatial Intelligence is our target variable, classified into five ordinal categories ranging from Very Low (VL) to Very High (VH). Understanding the distribution of this outcome variable is crucial for identifying potential class imbalance issues and informing our choice of evaluation metrics and sampling strategies. We'll visualize this distribution through multiple perspectives: raw counts, percentages, and normalized visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc87be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the distribution of the target variable\n",
    "print(\"=\" * 80)\n",
    "print(\"TARGET VARIABLE DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "target_distribution = df[target_col].value_counts().sort_index()\n",
    "target_percentages = (target_distribution / len(df)) * 100\n",
    "\n",
    "distribution_df = pd.DataFrame({\n",
    "    'Spatial_Intelligence_Level': target_distribution.index,\n",
    "    'Count': target_distribution.values,\n",
    "    'Percentage': target_percentages.values,\n",
    "    'Cumulative_Percentage': target_percentages.cumsum().values\n",
    "})\n",
    "\n",
    "print(\"\\nTarget Variable Distribution:\")\n",
    "display(distribution_df)\n",
    "\n",
    "# Calculate class balance metrics\n",
    "class_weights = len(df) / (len(target_distribution) * target_distribution)\n",
    "print(f\"\\nClass Balance Weights (for handling imbalance):\")\n",
    "for level, weight in zip(target_distribution.index, class_weights):\n",
    "    print(f\"  {level}: {weight:.4f}\")\n",
    "\n",
    "# Create visualization of target distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot with counts\n",
    "axes[0].bar(target_distribution.index, target_distribution.values, color=sns.color_palette(\"husl\", len(target_distribution)))\n",
    "axes[0].set_xlabel('Spatial Intelligence Level', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Distribution of Spatial Intelligence (Counts)', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(target_distribution.values):\n",
    "    axes[0].text(i, v + 1, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# Pie chart with percentages\n",
    "colors = sns.color_palette(\"husl\", len(target_distribution))\n",
    "wedges, texts, autotexts = axes[1].pie(\n",
    "    target_distribution.values,\n",
    "    labels=target_distribution.index,\n",
    "    autopct='%1.1f%%',\n",
    "    colors=colors,\n",
    "    startangle=90\n",
    ")\n",
    "axes[1].set_title('Distribution of Spatial Intelligence (Percentage)', fontsize=13, fontweight='bold')\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('white')\n",
    "    autotext.set_fontweight('bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/01_target_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Target distribution visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7594f6bf",
   "metadata": {},
   "source": [
    "## 2.2: Feature Type Classification and Statistical Profiling\n",
    "\n",
    "Before conducting deeper analysis, we need to classify features by type (numeric, categorical ordinal, categorical nominal) and understand their individual distributions. This classification will inform our encoding strategies and feature engineering decisions. We'll compute key statistical properties including skewness, kurtosis, and outlier presence for numeric features, and frequency distributions for categorical features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
