{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30b0da91",
   "metadata": {},
   "source": [
    "# SpatialIQ: Predicting Students' Spatial Intelligence through Machine Learning\n",
    "\n",
    "## A Comprehensive AI-in-Education Research Study\n",
    "\n",
    "**Author:** AI Research Developer  \n",
    "**Date:** November 2025  \n",
    "**Dataset Citation:** DOI: 10.21227/5qxw-bw66  \n",
    "**Purpose:** Interpretable prediction and analysis of spatial intelligence in high school students using behavioral, academic, and demographic features\n",
    "\n",
    "---\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "This notebook presents a rigorous machine learning pipeline designed to predict students' **Spatial Intelligence** levels (ranging from Very Low to Very High) using a rich dataset of 40 behavioral, academic, and demographic features collected from high school students. Spatial intelligence—the ability to visualize, manipulate, and reason about spatial relationships—is a critical component of cognitive development and academic achievement, particularly in STEM disciplines.\n",
    "\n",
    "### Research Questions:\n",
    "1. **What behavioral and academic patterns predict spatial intelligence?**\n",
    "2. **How do demographic factors (gender, parental education, environment) influence spatial reasoning ability?**\n",
    "3. **Can we build interpretable models that identify actionable insights for educators?**\n",
    "4. **What ethical considerations arise from AI-driven student cognitive profiling?**\n",
    "\n",
    "### Methodology Overview:\n",
    "- **Phase 1:** Comprehensive data exploration and statistical profiling\n",
    "- **Phase 2:** Intelligent feature engineering with domain knowledge\n",
    "- **Phase 3:** Competitive modeling with Logistic Regression, Random Forest, XGBoost, and Neural Networks\n",
    "- **Phase 4:** Deep model interpretation using SHAP explainability methods\n",
    "- **Phase 5:** Actionable insights and ethical considerations\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset Overview\n",
    "\n",
    "The dataset comprises 398 high school students with 40 features across five categories:\n",
    "- **Demographics:** Age, Gender, Class size, Environment (Urban/Suburban/Rural)\n",
    "- **Socioeconomic:** Family size, Parental occupation, Parental education, Income level\n",
    "- **Academic:** Major, GPA, Study time, Extra classes, Teacher assessment\n",
    "- **Behavioral:** Internet usage, TV watching, Pattern recognition, Geographic familiarity\n",
    "- **Gaming Preferences:** Action, Adventure, Strategy, Sport, Simulation, Role-playing, Puzzle games\n",
    "- **Learning Modes:** Visual vs. Auditory, Map usage, Diagram usage, Experience with GIS\n",
    "\n",
    "**Target Variable:** Spatial Intelligence (Categorical: VL, L, M, H, VH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d1d9e5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 1: Import Required Libraries and Dataset Loading\n",
    "\n",
    "This foundational section initializes all necessary libraries for data manipulation, visualization, machine learning, and model interpretation. We leverage industry-standard tools including pandas for data handling, scikit-learn for preprocessing and modeling, XGBoost for gradient boosting, TensorFlow for neural networks, and SHAP for model explainability. This comprehensive toolkit enables both rigorous statistical analysis and production-grade machine learning workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252c3951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== LIBRARY IMPORTS ====================\n",
    "# Core Data Science and Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Data Preprocessing and Encoding\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Model Selection, Training, and Evaluation\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Metrics and Evaluation\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score,\n",
    "    roc_curve, auc, matthews_corrcoef\n",
    ")\n",
    "\n",
    "# Statistical Analysis\n",
    "from scipy.stats import chi2_contingency, mutual_info_classif, spearmanr, pearsonr\n",
    "from scipy.stats import kurtosis, skew\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Feature Importance and SHAP Interpretability\n",
    "import shap\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Warnings and Configuration\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration for visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"✓ All libraries successfully imported\")\n",
    "print(f\"✓ NumPy version: {np.__version__}\")\n",
    "print(f\"✓ Pandas version: {pd.__version__}\")\n",
    "print(f\"✓ Scikit-learn version: {pd.__version__.split('.')[0]}\")\n",
    "print(f\"✓ SHAP library initialized for model interpretation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efe7f52",
   "metadata": {},
   "source": [
    "## 1.1: Load and Inspect the Dataset\n",
    "\n",
    "In this subsection, we load the Dataset.csv file and perform initial exploratory inspection to understand the data structure, dimensions, data types, and preliminary statistical characteristics. This foundational step is critical for identifying potential data quality issues, missing values, and the nature of features we'll be working with throughout the analysis."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
