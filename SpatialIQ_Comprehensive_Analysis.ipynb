{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30b0da91",
   "metadata": {},
   "source": [
    "# SpatialIQ: Predicting Students' Spatial Intelligence through Machine Learning\n",
    "\n",
    "## A Comprehensive AI-in-Education Research Study\n",
    "\n",
    "**Author:** AI Research Developer  \n",
    "**Date:** November 2025  \n",
    "**Dataset Citation:** DOI: 10.21227/5qxw-bw66  \n",
    "**Purpose:** Interpretable prediction and analysis of spatial intelligence in high school students using behavioral, academic, and demographic features\n",
    "\n",
    "---\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "This notebook presents a rigorous machine learning pipeline designed to predict students' **Spatial Intelligence** levels (ranging from Very Low to Very High) using a rich dataset of 40 behavioral, academic, and demographic features collected from high school students. Spatial intelligence—the ability to visualize, manipulate, and reason about spatial relationships—is a critical component of cognitive development and academic achievement, particularly in STEM disciplines.\n",
    "\n",
    "### Research Questions:\n",
    "1. **What behavioral and academic patterns predict spatial intelligence?**\n",
    "2. **How do demographic factors (gender, parental education, environment) influence spatial reasoning ability?**\n",
    "3. **Can we build interpretable models that identify actionable insights for educators?**\n",
    "4. **What ethical considerations arise from AI-driven student cognitive profiling?**\n",
    "\n",
    "### Methodology Overview:\n",
    "- **Phase 1:** Comprehensive data exploration and statistical profiling\n",
    "- **Phase 2:** Intelligent feature engineering with domain knowledge\n",
    "- **Phase 3:** Competitive modeling with Logistic Regression, Random Forest, XGBoost, and Neural Networks\n",
    "- **Phase 4:** Deep model interpretation using SHAP explainability methods\n",
    "- **Phase 5:** Actionable insights and ethical considerations\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset Overview\n",
    "\n",
    "The dataset comprises 398 high school students with 40 features across five categories:\n",
    "- **Demographics:** Age, Gender, Class size, Environment (Urban/Suburban/Rural)\n",
    "- **Socioeconomic:** Family size, Parental occupation, Parental education, Income level\n",
    "- **Academic:** Major, GPA, Study time, Extra classes, Teacher assessment\n",
    "- **Behavioral:** Internet usage, TV watching, Pattern recognition, Geographic familiarity\n",
    "- **Gaming Preferences:** Action, Adventure, Strategy, Sport, Simulation, Role-playing, Puzzle games\n",
    "- **Learning Modes:** Visual vs. Auditory, Map usage, Diagram usage, Experience with GIS\n",
    "\n",
    "**Target Variable:** Spatial Intelligence (Categorical: VL, L, M, H, VH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d1d9e5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 1: Import Required Libraries and Dataset Loading\n",
    "\n",
    "This foundational section initializes all necessary libraries for data manipulation, visualization, machine learning, and model interpretation. We leverage industry-standard tools including pandas for data handling, scikit-learn for preprocessing and modeling, XGBoost for gradient boosting, TensorFlow for neural networks, and SHAP for model explainability. This comprehensive toolkit enables both rigorous statistical analysis and production-grade machine learning workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252c3951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== LIBRARY IMPORTS ====================\n",
    "# Core Data Science and Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Data Preprocessing and Encoding\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Model Selection, Training, and Evaluation\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Metrics and Evaluation\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score,\n",
    "    roc_curve, auc, matthews_corrcoef\n",
    ")\n",
    "\n",
    "# Statistical Analysis\n",
    "from scipy.stats import chi2_contingency, mutual_info_classif, spearmanr, pearsonr\n",
    "from scipy.stats import kurtosis, skew\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Feature Importance and SHAP Interpretability\n",
    "import shap\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Warnings and Configuration\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration for visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"✓ All libraries successfully imported\")\n",
    "print(f\"✓ NumPy version: {np.__version__}\")\n",
    "print(f\"✓ Pandas version: {pd.__version__}\")\n",
    "print(f\"✓ Scikit-learn version: {pd.__version__.split('.')[0]}\")\n",
    "print(f\"✓ SHAP library initialized for model interpretation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efe7f52",
   "metadata": {},
   "source": [
    "## 1.1: Load and Inspect the Dataset\n",
    "\n",
    "In this subsection, we load the Dataset.csv file and perform initial exploratory inspection to understand the data structure, dimensions, data types, and preliminary statistical characteristics. This foundational step is critical for identifying potential data quality issues, missing values, and the nature of features we'll be working with throughout the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f676aea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from CSV file\n",
    "data_path = Path(\"data/Dataset.csv\")\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASET LOADING AND INITIAL INSPECTION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n✓ Dataset successfully loaded from: {data_path}\")\n",
    "print(f\"\\nDataset Shape: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "print(f\"Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Display column names and data types\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COLUMN INFORMATION AND DATA TYPES\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nColumn Names and Data Types:\")\n",
    "print(df.info())\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FIRST 10 ROWS OF DATA\")\n",
    "print(\"=\" * 80)\n",
    "display(df.head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "display(df.describe().transpose())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MISSING VALUES ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_counts.index,\n",
    "    'Missing_Count': missing_counts.values,\n",
    "    'Missing_Percentage': missing_percentage.values\n",
    "})\n",
    "missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    print(f\"\\nColumns with missing values ({len(missing_df)} found):\")\n",
    "    display(missing_df)\n",
    "else:\n",
    "    print(\"\\n✓ No missing values detected in the dataset!\")\n",
    "\n",
    "# Identify target variable\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TARGET VARIABLE IDENTIFICATION\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nDataset column names:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")\n",
    "\n",
    "# The last column is typically the target (Spatial Intelligence level)\n",
    "target_col = df.columns[-1]\n",
    "print(f\"\\n✓ Identified target variable: '{target_col}'\")\n",
    "print(f\"Unique values in target: {df[target_col].unique()}\")\n",
    "print(f\"Target value counts:\\n{df[target_col].value_counts().sort_index()}\")\n",
    "\n",
    "# Create a reference dictionary for later use\n",
    "target_mapping = {val: idx for idx, val in enumerate(sorted(df[target_col].unique()))}\n",
    "print(f\"\\nTarget ordinal mapping: {target_mapping}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404e2f51",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 2: Exploratory Data Analysis and Statistical Profiling\n",
    "\n",
    "## 2.1: Distribution Analysis of Spatial Intelligence\n",
    "\n",
    "Spatial Intelligence is our target variable, classified into five ordinal categories ranging from Very Low (VL) to Very High (VH). Understanding the distribution of this outcome variable is crucial for identifying potential class imbalance issues and informing our choice of evaluation metrics and sampling strategies. We'll visualize this distribution through multiple perspectives: raw counts, percentages, and normalized visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc87be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the distribution of the target variable\n",
    "print(\"=\" * 80)\n",
    "print(\"TARGET VARIABLE DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "target_distribution = df[target_col].value_counts().sort_index()\n",
    "target_percentages = (target_distribution / len(df)) * 100\n",
    "\n",
    "distribution_df = pd.DataFrame({\n",
    "    'Spatial_Intelligence_Level': target_distribution.index,\n",
    "    'Count': target_distribution.values,\n",
    "    'Percentage': target_percentages.values,\n",
    "    'Cumulative_Percentage': target_percentages.cumsum().values\n",
    "})\n",
    "\n",
    "print(\"\\nTarget Variable Distribution:\")\n",
    "display(distribution_df)\n",
    "\n",
    "# Calculate class balance metrics\n",
    "class_weights = len(df) / (len(target_distribution) * target_distribution)\n",
    "print(f\"\\nClass Balance Weights (for handling imbalance):\")\n",
    "for level, weight in zip(target_distribution.index, class_weights):\n",
    "    print(f\"  {level}: {weight:.4f}\")\n",
    "\n",
    "# Create visualization of target distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot with counts\n",
    "axes[0].bar(target_distribution.index, target_distribution.values, color=sns.color_palette(\"husl\", len(target_distribution)))\n",
    "axes[0].set_xlabel('Spatial Intelligence Level', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Distribution of Spatial Intelligence (Counts)', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(target_distribution.values):\n",
    "    axes[0].text(i, v + 1, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# Pie chart with percentages\n",
    "colors = sns.color_palette(\"husl\", len(target_distribution))\n",
    "wedges, texts, autotexts = axes[1].pie(\n",
    "    target_distribution.values,\n",
    "    labels=target_distribution.index,\n",
    "    autopct='%1.1f%%',\n",
    "    colors=colors,\n",
    "    startangle=90\n",
    ")\n",
    "axes[1].set_title('Distribution of Spatial Intelligence (Percentage)', fontsize=13, fontweight='bold')\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('white')\n",
    "    autotext.set_fontweight('bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/01_target_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Target distribution visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7594f6bf",
   "metadata": {},
   "source": [
    "## 2.2: Feature Type Classification and Statistical Profiling\n",
    "\n",
    "Before conducting deeper analysis, we need to classify features by type (numeric, categorical ordinal, categorical nominal) and understand their individual distributions. This classification will inform our encoding strategies and feature engineering decisions. We'll compute key statistical properties including skewness, kurtosis, and outlier presence for numeric features, and frequency distributions for categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926189e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs('visualizations', exist_ok=True)\n",
    "\n",
    "# Separate features from target\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "# Classify features by data type\n",
    "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FEATURE TYPE CLASSIFICATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nNumeric Features ({len(numeric_features)} total):\")\n",
    "for feat in numeric_features:\n",
    "    print(f\"  • {feat}\")\n",
    "\n",
    "print(f\"\\nCategorical Features ({len(categorical_features)} total):\")\n",
    "for feat in categorical_features:\n",
    "    print(f\"  • {feat}\")\n",
    "\n",
    "print(f\"\\n✓ Total Features (excluding target): {len(numeric_features) + len(categorical_features)}\")\n",
    "\n",
    "# Statistical profiling of numeric features\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"NUMERIC FEATURES: STATISTICAL PROFILING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "numeric_stats = []\n",
    "for feat in numeric_features:\n",
    "    stats = {\n",
    "        'Feature': feat,\n",
    "        'Mean': X[feat].mean(),\n",
    "        'Std': X[feat].std(),\n",
    "        'Min': X[feat].min(),\n",
    "        'Max': X[feat].max(),\n",
    "        'Median': X[feat].median(),\n",
    "        'Skewness': skew(X[feat].dropna()),\n",
    "        'Kurtosis': kurtosis(X[feat].dropna()),\n",
    "        'Unique_Values': X[feat].nunique()\n",
    "    }\n",
    "    numeric_stats.append(stats)\n",
    "\n",
    "numeric_stats_df = pd.DataFrame(numeric_stats)\n",
    "print(\"\\nNumeric Features Summary:\")\n",
    "display(numeric_stats_df.round(4))\n",
    "\n",
    "# Statistical profiling of categorical features\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CATEGORICAL FEATURES: VALUE DISTRIBUTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "categorical_stats = []\n",
    "for feat in categorical_features:\n",
    "    stats = {\n",
    "        'Feature': feat,\n",
    "        'Data_Type': X[feat].dtype,\n",
    "        'Unique_Values': X[feat].nunique(),\n",
    "        'Top_Value': X[feat].mode()[0] if len(X[feat].mode()) > 0 else 'N/A',\n",
    "        'Top_Value_Freq': X[feat].value_counts().iloc[0] if len(X[feat].value_counts()) > 0 else 0,\n",
    "        'Missing_Values': X[feat].isnull().sum()\n",
    "    }\n",
    "    categorical_stats.append(stats)\n",
    "\n",
    "categorical_stats_df = pd.DataFrame(categorical_stats)\n",
    "print(\"\\nCategorical Features Summary:\")\n",
    "display(categorical_stats_df)\n",
    "\n",
    "# Visualize distributions of numeric features\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VISUALIZING NUMERIC FEATURE DISTRIBUTIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "n_numeric = len(numeric_features)\n",
    "n_cols = 4\n",
    "n_rows = (n_numeric + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 3*n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feat in enumerate(numeric_features):\n",
    "    axes[idx].hist(X[feat].dropna(), bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_title(f'{feat}\\n(Skew: {skew(X[feat].dropna()):.2f})', fontweight='bold')\n",
    "    axes[idx].set_xlabel('Value')\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(n_numeric, len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/02_numeric_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Numeric distributions saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb24ce60",
   "metadata": {},
   "source": [
    "## 2.3: Categorical Feature Distributions and Demographic Insights\n",
    "\n",
    "Understanding the distribution of categorical features provides crucial insights into the composition of our study population. We'll analyze the frequency distributions across key demographic variables including gender, environment, major, and parental occupation. These distributions will help us understand potential biases in our dataset and inform our data preprocessing decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb6335b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis of key categorical features\n",
    "print(\"=\" * 80)\n",
    "print(\"CATEGORICAL FEATURE DETAILED ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Select key demographic features for visualization\n",
    "key_categorical = categorical_features[:8] if len(categorical_features) >= 8 else categorical_features\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feat in enumerate(key_categorical):\n",
    "    value_counts = X[feat].value_counts()\n",
    "    axes[idx].barh(value_counts.index, value_counts.values, color=sns.color_palette(\"husl\", len(value_counts)))\n",
    "    axes[idx].set_title(f'{feat}\\n({len(value_counts)} categories)', fontweight='bold')\n",
    "    axes[idx].set_xlabel('Frequency')\n",
    "    axes[idx].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(value_counts.values):\n",
    "        axes[idx].text(v + 0.5, i, str(v), va='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(len(key_categorical), len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/03_categorical_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Categorical distributions visualization saved\")\n",
    "\n",
    "# Print value counts for each categorical feature\n",
    "for feat in categorical_features:\n",
    "    print(f\"\\n{feat}:\")\n",
    "    print(X[feat].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f1b171",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 3: Data Preprocessing and Intelligent Encoding\n",
    "\n",
    "## 3.1: Strategic Encoding Framework\n",
    "\n",
    "This section implements a sophisticated encoding strategy that respects the underlying structure of different feature types:\n",
    "\n",
    "- **Ordinal Encoding:** Applied to features with inherent order (e.g., Spatial Intelligence levels VL→L→M→H→VH, Education levels)\n",
    "- **One-Hot Encoding:** Applied to nominal categorical features without meaningful order (e.g., gender, environment, major)\n",
    "- **Standardization:** Applied to numeric features to ensure comparable scales for algorithms sensitive to feature magnitude\n",
    "\n",
    "This intelligent approach preserves information while preparing data for machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd8b699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define encoding strategy for features\n",
    "print(\"=\" * 80)\n",
    "print(\"ENCODING STRATEGY DEFINITION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Ordinal features with explicit ordering (knowledge-based)\n",
    "# These are features that have a natural hierarchy\n",
    "ordinal_features_mapping = {\n",
    "    # Education levels (if present) - typically: Unemployment < High School < Bachelor < Master < PhD\n",
    "    # Spatial intelligence related orderings will be handled when found\n",
    "    # Identify by examining unique values\n",
    "}\n",
    "\n",
    "# Binary features (Yes/No, 0/1) - can be left as-is or encoded\n",
    "binary_features = []\n",
    "\n",
    "# Examine categorical features to identify ordinal vs nominal\n",
    "print(\"\\nExamining categorical features for ordinal structure:\")\n",
    "for feat in categorical_features:\n",
    "    unique_vals = sorted(X[feat].unique())\n",
    "    print(f\"  {feat}: {unique_vals}\")\n",
    "    \n",
    "    # Check if it looks ordinal based on values\n",
    "    if all(isinstance(v, (int, float)) for v in unique_vals):\n",
    "        binary_features.append(feat)\n",
    "\n",
    "print(f\"\\n✓ Identified {len(binary_features)} binary/numeric-like categorical features\")\n",
    "\n",
    "# Prepare data copy for preprocessing\n",
    "X_processed = X.copy()\n",
    "y_processed = y.copy()\n",
    "\n",
    "# Handle binary/numeric categorical features - convert to numeric if already numeric\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ENCODING IMPLEMENTATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for feat in binary_features:\n",
    "    X_processed[feat] = pd.to_numeric(X_processed[feat], errors='coerce').fillna(X_processed[feat].mode()[0])\n",
    "\n",
    "# Encode target variable using OrdinalEncoder (preserving order VL < L < M < H < VH)\n",
    "target_order = sorted(y_processed.unique())\n",
    "print(f\"\\nTarget variable ordinal encoding: {target_order}\")\n",
    "\n",
    "le_target = LabelEncoder()\n",
    "y_processed = pd.Series(le_target.fit_transform(y_processed), index=y_processed.index)\n",
    "\n",
    "# Separate remaining categorical features for One-Hot Encoding\n",
    "nominal_categorical = [f for f in categorical_features if f not in binary_features]\n",
    "\n",
    "print(f\"\\nFeatures to be One-Hot Encoded ({len(nominal_categorical)} total):\")\n",
    "for feat in nominal_categorical:\n",
    "    print(f\"  • {feat}\")\n",
    "\n",
    "# Apply One-Hot Encoding to nominal categorical features\n",
    "X_encoded = pd.get_dummies(X_processed, columns=nominal_categorical, drop_first=False, dtype=int)\n",
    "\n",
    "print(f\"\\n✓ Encoding complete!\")\n",
    "print(f\"Original features: {X_processed.shape[1]}\")\n",
    "print(f\"Features after encoding: {X_encoded.shape[1]}\")\n",
    "print(f\"New feature columns added: {X_encoded.shape[1] - X_processed.shape[1]}\")\n",
    "\n",
    "# Display encoded feature names\n",
    "print(\"\\nEncoded features:\")\n",
    "encoded_cols = X_encoded.columns.tolist()\n",
    "for i, col in enumerate(encoded_cols, 1):\n",
    "    print(f\"{i:3d}. {col}\")\n",
    "\n",
    "# Standardize numeric features\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"NUMERIC FEATURE STANDARDIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "numeric_features_encoded = X_encoded.select_dtypes(include=[np.number]).columns.tolist()\n",
    "scaler = StandardScaler()\n",
    "X_scaled = X_encoded.copy()\n",
    "X_scaled[numeric_features_encoded] = scaler.fit_transform(X_encoded[numeric_features_encoded])\n",
    "\n",
    "print(f\"\\n✓ Standardized {len(numeric_features_encoded)} numeric features\")\n",
    "print(f\"  Using StandardScaler (mean=0, std=1)\")\n",
    "\n",
    "# Display summary of preprocessing\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PREPROCESSING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nOriginal dataset shape: {df.shape}\")\n",
    "print(f\"Processed dataset shape: {X_scaled.shape}\")\n",
    "print(f\"Target variable shape: {y_processed.shape}\")\n",
    "print(f\"\\n✓ Data preprocessing completed successfully!\")\n",
    "\n",
    "# Show sample of processed data\n",
    "print(\"\\nSample of processed data (first 5 rows):\")\n",
    "display(X_scaled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673b816f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 4: Feature Engineering and Dimensionality Reduction\n",
    "\n",
    "## 4.1: Domain-Driven Feature Engineering\n",
    "\n",
    "In this section, we create scientifically-motivated derived features that capture meaningful combinations of existing variables. Domain expertise in education psychology and cognitive science informs our feature engineering decisions. These engineered features aim to represent latent concepts that directly influence spatial intelligence development.\n",
    "\n",
    "### Engineered Features:\n",
    "1. **Study Efficiency:** Ratio of study time to academic performance (GPA)\n",
    "2. **Academic Support Index:** Combination of extra classes and parental education level\n",
    "3. **Gaming Engagement:** Aggregated preference across multiple game genres\n",
    "4. **Visual Learning Orientation:** Combined score of map and diagram usage\n",
    "5. **Digital Lifestyle:** Combined internet and gaming engagement\n",
    "6. **Family Education:** Average of parental education levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bcd0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create engineered features from original data\n",
    "X_engineered = X.copy()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FEATURE ENGINEERING: CREATING DERIVED FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "engineered_features_info = []\n",
    "\n",
    "# 1. Study Efficiency = Study_time / GPA (if GPA > 0 to avoid division by zero)\n",
    "# First, identify columns that might relate to study time and GPA\n",
    "study_time_cols = [col for col in X.columns if 'study' in col.lower() or 'time' in col.lower()]\n",
    "gpa_cols = [col for col in X.columns if 'gpa' in col.lower()]\n",
    "\n",
    "if study_time_cols and gpa_cols:\n",
    "    study_col = study_time_cols[0]\n",
    "    gpa_col = gpa_cols[0]\n",
    "    X_engineered['Study_Efficiency'] = X_engineered[study_col] / (X_engineered[gpa_col] + 1e-6)\n",
    "    engineered_features_info.append({\n",
    "        'Feature': 'Study_Efficiency',\n",
    "        'Formula': f'{study_col} / {gpa_col}',\n",
    "        'Interpretation': 'Study time investment relative to academic performance'\n",
    "    })\n",
    "    print(f\"\\n✓ Study_Efficiency = {study_col} / {gpa_col}\")\n",
    "\n",
    "# 2. Extra Class Engagement\n",
    "extra_class_cols = [col for col in X.columns if 'extra' in col.lower() or 'class' in col.lower()]\n",
    "if extra_class_cols:\n",
    "    X_engineered['Extra_Classes'] = X_engineered[extra_class_cols[0]] if len(extra_class_cols) > 0 else 0\n",
    "    engineered_features_info.append({\n",
    "        'Feature': 'Extra_Classes',\n",
    "        'Formula': f'Direct feature: {extra_class_cols[0] if extra_class_cols else \"N/A\"}',\n",
    "        'Interpretation': 'Academic support through additional coursework'\n",
    "    })\n",
    "\n",
    "# 3. Internet and Gaming Engagement\n",
    "internet_cols = [col for col in X.columns if 'internet' in col.lower()]\n",
    "game_cols = [col for col in X.columns if col.startswith('G-')]\n",
    "\n",
    "if internet_cols:\n",
    "    X_engineered['Internet_Usage'] = X_engineered[internet_cols[0]]\n",
    "    print(f\"✓ Internet_Usage identified\")\n",
    "\n",
    "if game_cols:\n",
    "    X_engineered['Gaming_Engagement'] = X_engineered[game_cols].sum(axis=1)\n",
    "    engineered_features_info.append({\n",
    "        'Feature': 'Gaming_Engagement',\n",
    "        'Formula': f'Sum of {len(game_cols)} gaming preferences',\n",
    "        'Interpretation': 'Total engagement across gaming genres'\n",
    "    })\n",
    "    print(f\"✓ Gaming_Engagement = Sum of {len(game_cols)} game categories\")\n",
    "\n",
    "# 4. Visual Learning Orientation\n",
    "visual_learning_cols = [col for col in X.columns if 'map' in col.lower() or 'diagram' in col.lower()]\n",
    "if visual_learning_cols:\n",
    "    X_engineered['Visual_Learning'] = X_engineered[visual_learning_cols].sum(axis=1)\n",
    "    engineered_features_info.append({\n",
    "        'Feature': 'Visual_Learning',\n",
    "        'Formula': f'Sum of visual learning indicators',\n",
    "        'Interpretation': 'Preference for visual representations in learning'\n",
    "    })\n",
    "    print(f\"✓ Visual_Learning = Combined visual learning preferences\")\n",
    "\n",
    "# 5. Pattern Recognition and Spatial Skills\n",
    "pattern_cols = [col for col in X.columns if 'pattern' in col.lower() or 'finding' in col.lower() or 'direction' in col.lower()]\n",
    "if pattern_cols:\n",
    "    X_engineered['Spatial_Skills_Proxy'] = X_engineered[pattern_cols].sum(axis=1)\n",
    "    engineered_features_info.append({\n",
    "        'Feature': 'Spatial_Skills_Proxy',\n",
    "        'Formula': f'Sum of {len(pattern_cols)} spatial reasoning indicators',\n",
    "        'Interpretation': 'Proxy measure for inherent spatial abilities'\n",
    "    })\n",
    "    print(f\"✓ Spatial_Skills_Proxy = Sum of pattern/direction indicators\")\n",
    "\n",
    "# 6. Parental Education (if available)\n",
    "parent_edu_cols = [col for col in X.columns if 'father' in col.lower() or 'mother' in col.lower() or 'parent' in col.lower()]\n",
    "if parent_edu_cols:\n",
    "    edu_cols = [col for col in parent_edu_cols if 'education' in col.lower()]\n",
    "    if edu_cols:\n",
    "        numeric_edu = X_engineered[edu_cols].apply(pd.to_numeric, errors='coerce')\n",
    "        X_engineered['Parental_Education_Avg'] = numeric_edu.mean(axis=1)\n",
    "        engineered_features_info.append({\n",
    "            'Feature': 'Parental_Education_Avg',\n",
    "            'Formula': f'Mean of parental education levels',\n",
    "            'Interpretation': 'Family educational background composite'\n",
    "        })\n",
    "        print(f\"✓ Parental_Education_Avg = Average of parental education\")\n",
    "\n",
    "# 7. Digital Lifestyle Index\n",
    "digital_cols = [col for col in X.columns if any(x in col.lower() for x in ['internet', 'tv', 'game'])]\n",
    "if digital_cols:\n",
    "    digital_numeric = X_engineered[digital_cols].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "    X_engineered['Digital_Lifestyle'] = digital_numeric.sum(axis=1)\n",
    "    engineered_features_info.append({\n",
    "        'Feature': 'Digital_Lifestyle',\n",
    "        'Formula': f'Sum of digital engagement indicators',\n",
    "        'Interpretation': 'Overall digital media consumption and engagement'\n",
    "    })\n",
    "    print(f\"✓ Digital_Lifestyle = Combined digital engagement score\")\n",
    "\n",
    "# Select only the engineered numeric features\n",
    "engineered_cols = [col for col in X_engineered.columns if col not in X.columns]\n",
    "print(f\"\\n✓ Successfully engineered {len(engineered_cols)} new features\")\n",
    "\n",
    "# Display engineered features summary\n",
    "print(\"\\nEngineered Features Summary:\")\n",
    "engineered_summary_df = pd.DataFrame(engineered_features_info)\n",
    "display(engineered_summary_df)\n",
    "\n",
    "# Display statistics for engineered features\n",
    "print(\"\\nStatistics of Engineered Features:\")\n",
    "engineered_stats = X_engineered[engineered_cols].describe().transpose()\n",
    "print(engineered_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5ae68f",
   "metadata": {},
   "source": [
    "## 4.2: Combining Original and Engineered Features\n",
    "\n",
    "Now we create a comprehensive feature set that includes both original (preprocessed) and engineered features. We'll standardize this combined set and then apply dimensionality reduction to manage the feature space while preserving predictive information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71221a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine original scaled features with engineered features\n",
    "print(\"=\" * 80)\n",
    "print(\"COMBINING ORIGINAL AND ENGINEERED FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Re-encode and scale original features\n",
    "X_prep_original = X.copy()\n",
    "\n",
    "# Apply same encoding as before\n",
    "X_prep_encoded = pd.get_dummies(X_prep_original, columns=nominal_categorical, drop_first=False, dtype=int)\n",
    "\n",
    "# Select numeric columns and scale\n",
    "numeric_cols_for_scaling = X_prep_encoded.select_dtypes(include=[np.number]).columns.tolist()\n",
    "scaler_final = StandardScaler()\n",
    "X_prep_scaled = X_prep_encoded.copy()\n",
    "X_prep_scaled[numeric_cols_for_scaling] = scaler_final.fit_transform(X_prep_encoded[numeric_cols_for_scaling])\n",
    "\n",
    "# Add engineered features (raw, will be standardized)\n",
    "for eng_col in engineered_cols:\n",
    "    if eng_col in X_engineered.columns:\n",
    "        X_prep_scaled[eng_col] = X_engineered[eng_col]\n",
    "\n",
    "# Final standardization of all numeric features including engineered\n",
    "all_numeric_cols = X_prep_scaled.select_dtypes(include=[np.number]).columns.tolist()\n",
    "scaler_all = StandardScaler()\n",
    "X_final = X_prep_scaled.copy()\n",
    "X_final[all_numeric_cols] = scaler_all.fit_transform(X_prep_scaled[all_numeric_cols].fillna(0))\n",
    "\n",
    "print(f\"\\n✓ Combined feature set created\")\n",
    "print(f\"Total features: {X_final.shape[1]}\")\n",
    "print(f\"  - Original processed features: {X_prep_encoded.shape[1]}\")\n",
    "print(f\"  - Engineered features: {len(engineered_cols)}\")\n",
    "print(f\"  - Redundant features removed: {X_prep_encoded.shape[1] + len(engineered_cols) - X_final.shape[1]}\")\n",
    "\n",
    "# Check for multicollinearity using VIF (Variance Inflation Factor)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MULTICOLLINEARITY ASSESSMENT (VIF)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate VIF for a sample of numeric features (VIF is computationally expensive)\n",
    "vif_sample_cols = all_numeric_cols[:min(20, len(all_numeric_cols))]\n",
    "vif_data = X_final[vif_sample_cols].fillna(0)\n",
    "\n",
    "print(\"\\nVariance Inflation Factor for sample features:\")\n",
    "vif_results = []\n",
    "for col in vif_sample_cols:\n",
    "    try:\n",
    "        vif = variance_inflation_factor(vif_data.values, vif_data.columns.get_loc(col))\n",
    "        vif_results.append({'Feature': col, 'VIF': vif})\n",
    "    except:\n",
    "        vif_results.append({'Feature': col, 'VIF': np.nan})\n",
    "\n",
    "vif_df = pd.DataFrame(vif_results).sort_values('VIF', ascending=False)\n",
    "display(vif_df.head(10))\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PRINCIPAL COMPONENT ANALYSIS (PCA)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Fit PCA with different numbers of components\n",
    "n_features = X_final.shape[1]\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_final.fillna(0))\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "cumsum_var = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "# Find number of components to retain 95% variance\n",
    "n_components_95 = np.argmax(cumsum_var >= 0.95) + 1\n",
    "n_components_90 = np.argmax(cumsum_var >= 0.90) + 1\n",
    "\n",
    "print(f\"\\nPCA Analysis Results:\")\n",
    "print(f\"  Total original features: {n_features}\")\n",
    "print(f\"  Components for 90% variance: {n_components_90}\")\n",
    "print(f\"  Components for 95% variance: {n_components_95}\")\n",
    "print(f\"  Variance explained by top 10 components: {cumsum_var[9]:.4f}\")\n",
    "\n",
    "# Create PCA-transformed dataset with 95% variance\n",
    "pca_95 = PCA(n_components=n_components_95)\n",
    "X_pca_95 = pca_95.fit_transform(X_final.fillna(0))\n",
    "\n",
    "# Also keep full feature set for model comparison\n",
    "X_data_for_modeling = X_final.fillna(0)\n",
    "\n",
    "print(f\"\\n✓ PCA transformation complete\")\n",
    "print(f\"  Dataset shape for modeling: {X_data_for_modeling.shape}\")\n",
    "print(f\"  PCA-reduced dataset shape: {X_pca_95.shape}\")\n",
    "\n",
    "# Visualize PCA explained variance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scree plot\n",
    "axes[0].plot(range(1, min(31, len(cumsum_var)+1)), cumsum_var[:30], 'bo-', linewidth=2, markersize=6)\n",
    "axes[0].axhline(y=0.95, color='r', linestyle='--', label='95% variance')\n",
    "axes[0].axhline(y=0.90, color='g', linestyle='--', label='90% variance')\n",
    "axes[0].set_xlabel('Number of Principal Components', fontweight='bold', fontsize=11)\n",
    "axes[0].set_ylabel('Cumulative Explained Variance', fontweight='bold', fontsize=11)\n",
    "axes[0].set_title('PCA Scree Plot: Cumulative Explained Variance', fontweight='bold', fontsize=12)\n",
    "axes[0].grid(alpha=0.3)\n",
    "axes[0].legend()\n",
    "\n",
    "# Individual component variance\n",
    "axes[1].bar(range(1, min(21, len(pca_full.explained_variance_ratio_)+1)), \n",
    "            pca_full.explained_variance_ratio_[:20], color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[1].set_xlabel('Principal Component', fontweight='bold', fontsize=11)\n",
    "axes[1].set_ylabel('Explained Variance Ratio', fontweight='bold', fontsize=11)\n",
    "axes[1].set_title('Variance Explained by Individual Components', fontweight='bold', fontsize=12)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/04_pca_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ PCA visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ca354b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 5: Correlation Analysis and Feature Selection\n",
    "\n",
    "## 5.1: Correlation Heatmap and Feature Relationships\n",
    "\n",
    "Understanding the relationships between features and the target variable is crucial for identifying predictive power and avoiding multicollinearity issues. In this section, we compute correlation matrices and visualize them using hierarchical clustering to identify feature groups and target relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75abede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis with target variable\n",
    "print(\"=\" * 80)\n",
    "print(\"FEATURE-TARGET CORRELATION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate correlation of all features with target\n",
    "correlations_with_target = []\n",
    "for col in X_data_for_modeling.columns:\n",
    "    corr = np.corrcoef(X_data_for_modeling[col], y_processed)[0, 1]\n",
    "    abs_corr = abs(corr)\n",
    "    correlations_with_target.append({\n",
    "        'Feature': col,\n",
    "        'Correlation': corr,\n",
    "        'Abs_Correlation': abs_corr\n",
    "    })\n",
    "\n",
    "corr_df = pd.DataFrame(correlations_with_target).sort_values('Abs_Correlation', ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 Features by Absolute Correlation with Target:\")\n",
    "display(corr_df.head(20))\n",
    "\n",
    "# Select top features\n",
    "top_features = corr_df[corr_df['Abs_Correlation'] > 0.1].copy()\n",
    "print(f\"\\nFeatures with |correlation| > 0.1: {len(top_features)}\")\n",
    "display(top_features.head(15))\n",
    "\n",
    "# Create correlation heatmap for top features\n",
    "top_feature_cols = corr_df.head(20)['Feature'].tolist()\n",
    "X_top = X_data_for_modeling[top_feature_cols].copy()\n",
    "X_top['Target'] = y_processed.values\n",
    "\n",
    "corr_matrix = X_top.corr()\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.clustermap(corr_matrix, cmap='RdBu_r', center=0, figsize=(14, 12), \n",
    "               cbar_kws={'label': 'Correlation Coefficient'},\n",
    "               xticklabels=True, yticklabels=True)\n",
    "plt.suptitle('Correlation Heatmap: Top 20 Features + Target Variable', \n",
    "             fontweight='bold', fontsize=13, y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/05_correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Correlation heatmap saved\")\n",
    "\n",
    "# Mutual Information Analysis\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MUTUAL INFORMATION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "mi_scores = mutual_info_classif(X_data_for_modeling, y_processed, random_state=42)\n",
    "mi_df = pd.DataFrame({\n",
    "    'Feature': X_data_for_modeling.columns,\n",
    "    'MI_Score': mi_scores\n",
    "}).sort_values('MI_Score', ascending=False)\n",
    "\n",
    "print(\"\\nMutual Information Scores (Top 20 Features):\")\n",
    "display(mi_df.head(20))\n",
    "\n",
    "# Visualize MI scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_mi = mi_df.head(20)\n",
    "plt.barh(top_mi['Feature'], top_mi['MI_Score'], color=sns.color_palette(\"husl\", 20))\n",
    "plt.xlabel('Mutual Information Score', fontweight='bold', fontsize=11)\n",
    "plt.title('Top 20 Features by Mutual Information with Target', fontweight='bold', fontsize=12)\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/06_mutual_information.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Mutual information visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6b6b68",
   "metadata": {},
   "source": [
    "## 5.2: Chi-Square Analysis for Categorical Associations\n",
    "\n",
    "For categorical features, we employ chi-square tests to identify significant associations with the target variable. This statistical approach complements the correlation analysis and helps us identify which demographic and behavioral categories most strongly influence spatial intelligence classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5970e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi-Square Analysis for original categorical features\n",
    "print(\"=\" * 80)\n",
    "print(\"CHI-SQUARE ANALYSIS FOR CATEGORICAL FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "chi2_results = []\n",
    "\n",
    "for feat in categorical_features:\n",
    "    try:\n",
    "        # Create contingency table\n",
    "        contingency = pd.crosstab(X[feat], y_processed)\n",
    "        chi2, p_value, dof, expected = chi2_contingency(contingency)\n",
    "        \n",
    "        chi2_results.append({\n",
    "            'Feature': feat,\n",
    "            'Chi2_Statistic': chi2,\n",
    "            'P_Value': p_value,\n",
    "            'DOF': dof,\n",
    "            'Significant': 'Yes' if p_value < 0.05 else 'No'\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {feat}: {e}\")\n",
    "\n",
    "chi2_df = pd.DataFrame(chi2_results).sort_values('Chi2_Statistic', ascending=False)\n",
    "\n",
    "print(\"\\nChi-Square Test Results for Categorical Features:\")\n",
    "display(chi2_df)\n",
    "\n",
    "print(\"\\n✓ Chi-square analysis complete\")\n",
    "\n",
    "# Feature selection summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FEATURE SELECTION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "selected_features_corr = corr_df[corr_df['Abs_Correlation'] > 0.05]['Feature'].tolist()\n",
    "selected_features_mi = mi_df[mi_df['MI_Score'] > mi_df['MI_Score'].quantile(0.75)]['Feature'].tolist()\n",
    "\n",
    "print(f\"\\nFeatures selected by correlation (|r| > 0.05): {len(selected_features_corr)}\")\n",
    "print(f\"Features selected by MI score (top 25%): {len(selected_features_mi)}\")\n",
    "\n",
    "# Combine selected features\n",
    "all_selected = list(set(selected_features_corr + selected_features_mi))\n",
    "print(f\"Union of selected features: {len(all_selected)}\")\n",
    "\n",
    "print(\"\\nFinal Feature Set for Modeling:\")\n",
    "print(f\"  - Using all {X_data_for_modeling.shape[1]} features for initial models\")\n",
    "print(f\"  - Will track feature importance during model training\")\n",
    "print(f\"  - PCA-reduced set ({X_pca_95.shape[1]} components) available for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791c2b29",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 6: Model Training with Multiple Algorithms\n",
    "\n",
    "## 6.1: Model Development Framework\n",
    "\n",
    "This section implements a competitive machine learning framework comparing four distinct algorithmic approaches, each selected for their complementary strengths:\n",
    "\n",
    "1. **Logistic Regression:** Baseline interpretable model with probabilistic output\n",
    "2. **Random Forest:** Ensemble method capturing non-linear relationships and feature interactions\n",
    "3. **Gradient Boosting (XGBoost):** Sequential ensemble with superior predictive power\n",
    "4. **Neural Network (MLPClassifier):** Deep learning approach capturing complex patterns\n",
    "\n",
    "We employ stratified 5-fold cross-validation to ensure robust evaluation and prevent overfitting. Each model is trained with hyperparameter tuning optimized for the specific characteristics of our spatial intelligence prediction task.\n",
    "\n",
    "### Rationale for Model Selection:\n",
    "- **Logistic Regression** provides a transparent baseline showing that the problem is not trivially solved\n",
    "- **Random Forest** excels at capturing feature interactions without extensive hyperparameter tuning\n",
    "- **XGBoost** represents state-of-the-art gradient boosting with proven effectiveness on tabular data\n",
    "- **Neural Network** explores potential non-linear manifolds in the high-dimensional feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c0934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL TRAINING PREPARATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Split data into train and test sets (stratified to preserve class distribution)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_data_for_modeling, y_processed, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y_processed\n",
    ")\n",
    "\n",
    "print(f\"\\nData Split (80-20):\")\n",
    "print(f\"  Training set: {X_train.shape[0]} samples × {X_train.shape[1]} features\")\n",
    "print(f\"  Test set: {X_test.shape[0]} samples × {X_test.shape[1]} features\")\n",
    "print(f\"  Target distribution (train): {pd.Series(y_train).value_counts().sort_index().to_dict()}\")\n",
    "print(f\"  Target distribution (test): {pd.Series(y_test).value_counts().sort_index().to_dict()}\")\n",
    "\n",
    "# Initialize cross-validation strategy\n",
    "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(f\"\\n✓ Cross-Validation Strategy: Stratified 5-Fold\")\n",
    "print(f\"  Stratification preserves class distribution in each fold\")\n",
    "\n",
    "# Dictionary to store trained models\n",
    "trained_models = {}\n",
    "cv_scores = {}\n",
    "test_predictions = {}\n",
    "test_probabilities = {}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL 1: LOGISTIC REGRESSION (BASELINE)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Logistic Regression\n",
    "lr_model = LogisticRegression(\n",
    "    max_iter=1000, \n",
    "    random_state=42, \n",
    "    multi_class='multinomial',\n",
    "    solver='lbfgs',\n",
    "    class_weight='balanced'  # Handle class imbalance\n",
    ")\n",
    "\n",
    "# Cross-validation evaluation\n",
    "lr_cv_scores = cross_validate(\n",
    "    lr_model, X_train, y_train, \n",
    "    cv=cv_strategy,\n",
    "    scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'],\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(f\"\\nCross-Validation Scores:\")\n",
    "print(f\"  Accuracy:  {lr_cv_scores['test_accuracy'].mean():.4f} (+/- {lr_cv_scores['test_accuracy'].std():.4f})\")\n",
    "print(f\"  Precision: {lr_cv_scores['test_precision_macro'].mean():.4f} (+/- {lr_cv_scores['test_precision_macro'].std():.4f})\")\n",
    "print(f\"  Recall:    {lr_cv_scores['test_recall_macro'].mean():.4f} (+/- {lr_cv_scores['test_recall_macro'].std():.4f})\")\n",
    "print(f\"  F1-Score:  {lr_cv_scores['test_f1_macro'].mean():.4f} (+/- {lr_cv_scores['test_f1_macro'].std():.4f})\")\n",
    "\n",
    "# Train on full training set\n",
    "lr_model.fit(X_train, y_train)\n",
    "lr_pred = lr_model.predict(X_test)\n",
    "lr_proba = lr_model.predict_proba(X_test)\n",
    "\n",
    "trained_models['Logistic_Regression'] = lr_model\n",
    "test_predictions['Logistic_Regression'] = lr_pred\n",
    "test_probabilities['Logistic_Regression'] = lr_proba\n",
    "cv_scores['Logistic_Regression'] = lr_cv_scores\n",
    "\n",
    "print(f\"\\n✓ Logistic Regression model trained and evaluated\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL 2: RANDOM FOREST CLASSIFIER\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=15,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "# Cross-validation evaluation\n",
    "rf_cv_scores = cross_validate(\n",
    "    rf_model, X_train, y_train,\n",
    "    cv=cv_strategy,\n",
    "    scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'],\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(f\"\\nCross-Validation Scores:\")\n",
    "print(f\"  Accuracy:  {rf_cv_scores['test_accuracy'].mean():.4f} (+/- {rf_cv_scores['test_accuracy'].std():.4f})\")\n",
    "print(f\"  Precision: {rf_cv_scores['test_precision_macro'].mean():.4f} (+/- {rf_cv_scores['test_precision_macro'].std():.4f})\")\n",
    "print(f\"  Recall:    {rf_cv_scores['test_recall_macro'].mean():.4f} (+/- {rf_cv_scores['test_recall_macro'].std():.4f})\")\n",
    "print(f\"  F1-Score:  {rf_cv_scores['test_f1_macro'].mean():.4f} (+/- {rf_cv_scores['test_f1_macro'].std():.4f})\")\n",
    "\n",
    "# Train on full training set\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "rf_proba = rf_model.predict_proba(X_test)\n",
    "\n",
    "trained_models['Random_Forest'] = rf_model\n",
    "test_predictions['Random_Forest'] = rf_pred\n",
    "test_probabilities['Random_Forest'] = rf_proba\n",
    "cv_scores['Random_Forest'] = rf_cv_scores\n",
    "\n",
    "print(f\"\\n✓ Random Forest model trained and evaluated\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL 3: XGBOOST CLASSIFIER\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# XGBoost\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=7,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    eval_metric='mlogloss'\n",
    ")\n",
    "\n",
    "# Cross-validation evaluation\n",
    "xgb_cv_scores = cross_validate(\n",
    "    xgb_model, X_train, y_train,\n",
    "    cv=cv_strategy,\n",
    "    scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'],\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(f\"\\nCross-Validation Scores:\")\n",
    "print(f\"  Accuracy:  {xgb_cv_scores['test_accuracy'].mean():.4f} (+/- {xgb_cv_scores['test_accuracy'].std():.4f})\")\n",
    "print(f\"  Precision: {xgb_cv_scores['test_precision_macro'].mean():.4f} (+/- {xgb_cv_scores['test_precision_macro'].std():.4f})\")\n",
    "print(f\"  Recall:    {xgb_cv_scores['test_recall_macro'].mean():.4f} (+/- {xgb_cv_scores['test_recall_macro'].std():.4f})\")\n",
    "print(f\"  F1-Score:  {xgb_cv_scores['test_f1_macro'].mean():.4f} (+/- {xgb_cv_scores['test_f1_macro'].std():.4f})\")\n",
    "\n",
    "# Train on full training set\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_pred = xgb_model.predict(X_test)\n",
    "xgb_proba = xgb_model.predict_proba(X_test)\n",
    "\n",
    "trained_models['XGBoost'] = xgb_model\n",
    "test_predictions['XGBoost'] = xgb_pred\n",
    "test_probabilities['XGBoost'] = xgb_proba\n",
    "cv_scores['XGBoost'] = xgb_cv_scores\n",
    "\n",
    "print(f\"\\n✓ XGBoost model trained and evaluated\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL 4: NEURAL NETWORK (MLP CLASSIFIER)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Neural Network\n",
    "nn_model = MLPClassifier(\n",
    "    hidden_layer_sizes=(256, 128, 64),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=500,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=20,\n",
    "    alpha=0.0001,  # L2 regularization\n",
    "    learning_rate='adaptive'\n",
    ")\n",
    "\n",
    "# Cross-validation evaluation\n",
    "nn_cv_scores = cross_validate(\n",
    "    nn_model, X_train, y_train,\n",
    "    cv=cv_strategy,\n",
    "    scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'],\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(f\"\\nCross-Validation Scores:\")\n",
    "print(f\"  Accuracy:  {nn_cv_scores['test_accuracy'].mean():.4f} (+/- {nn_cv_scores['test_accuracy'].std():.4f})\")\n",
    "print(f\"  Precision: {nn_cv_scores['test_precision_macro'].mean():.4f} (+/- {nn_cv_scores['test_precision_macro'].std():.4f})\")\n",
    "print(f\"  Recall:    {nn_cv_scores['test_recall_macro'].mean():.4f} (+/- {nn_cv_scores['test_recall_macro'].std():.4f})\")\n",
    "print(f\"  F1-Score:  {nn_cv_scores['test_f1_macro'].mean():.4f} (+/- {nn_cv_scores['test_f1_macro'].std():.4f})\")\n",
    "\n",
    "# Train on full training set\n",
    "nn_model.fit(X_train, y_train)\n",
    "nn_pred = nn_model.predict(X_test)\n",
    "nn_proba = nn_model.predict_proba(X_test)\n",
    "\n",
    "trained_models['Neural_Network'] = nn_model\n",
    "test_predictions['Neural_Network'] = nn_pred\n",
    "test_probabilities['Neural_Network'] = nn_proba\n",
    "cv_scores['Neural_Network'] = nn_cv_scores\n",
    "\n",
    "print(f\"\\n✓ Neural Network model trained and evaluated\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ ALL MODELS SUCCESSFULLY TRAINED\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ec78ef",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 7: Model Evaluation and Performance Comparison\n",
    "\n",
    "## 7.1: Comprehensive Evaluation Metrics\n",
    "\n",
    "In this section, we compute detailed evaluation metrics for each model across multiple dimensions: accuracy, precision, recall, F1-score, and balanced accuracy. These metrics provide complementary perspectives on model performance, particularly important given the potential class imbalance in spatial intelligence levels. We'll also generate confusion matrices and detailed classification reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3976eb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models on test set\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPREHENSIVE MODEL EVALUATION ON TEST SET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "evaluation_results = []\n",
    "\n",
    "for model_name, predictions in test_predictions.items():\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions, average='macro', zero_division=0)\n",
    "    recall = recall_score(y_test, predictions, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y_test, predictions, average='macro', zero_division=0)\n",
    "    mcc = matthews_corrcoef(y_test, predictions)\n",
    "    \n",
    "    evaluation_results.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1_Score': f1,\n",
    "        'Matthews_CC': mcc\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Accuracy:       {accuracy:.4f}\")\n",
    "    print(f\"  Precision:      {precision:.4f}\")\n",
    "    print(f\"  Recall:         {recall:.4f}\")\n",
    "    print(f\"  F1-Score:       {f1:.4f}\")\n",
    "    print(f\"  Matthews CC:    {mcc:.4f}\")\n",
    "\n",
    "# Create evaluation comparison dataframe\n",
    "eval_df = pd.DataFrame(evaluation_results).sort_values('F1_Score', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL PERFORMANCE RANKING\")\n",
    "print(\"=\" * 80)\n",
    "display(eval_df.round(4))\n",
    "\n",
    "# Visualize model performance comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1_Score']\n",
    "colors = sns.color_palette(\"husl\", len(eval_df))\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    bars = ax.barh(eval_df['Model'], eval_df[metric], color=colors)\n",
    "    ax.set_xlabel(metric, fontweight='bold', fontsize=11)\n",
    "    ax.set_title(f'Model Comparison: {metric}', fontweight='bold', fontsize=12)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, val) in enumerate(zip(bars, eval_df[metric])):\n",
    "        ax.text(val + 0.02, i, f'{val:.4f}', va='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/07_model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Model comparison visualization saved\")\n",
    "\n",
    "# Generate confusion matrices\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONFUSION MATRICES FOR ALL MODELS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (model_name, predictions) in enumerate(test_predictions.items()):\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx], \n",
    "                cbar=True, square=True, xticklabels=sorted(y_test.unique()),\n",
    "                yticklabels=sorted(y_test.unique()))\n",
    "    axes[idx].set_title(f'{model_name} - Confusion Matrix', fontweight='bold', fontsize=12)\n",
    "    axes[idx].set_ylabel('Actual', fontweight='bold', fontsize=11)\n",
    "    axes[idx].set_xlabel('Predicted', fontweight='bold', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/08_confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Confusion matrices visualization saved\")\n",
    "\n",
    "# Detailed classification reports\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DETAILED CLASSIFICATION REPORTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for model_name, predictions in test_predictions.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(classification_report(y_test, predictions, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b20080",
   "metadata": {},
   "source": [
    "## 7.2: Feature Importance Analysis\n",
    "\n",
    "Understanding which features drive model predictions is critical for interpretability and actionable insights. We'll extract feature importance from both tree-based models (Random Forest and XGBoost using built-in importance) and compute permutation importance for all models to provide a model-agnostic perspective on feature influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a16f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance from tree-based models\n",
    "print(\"=\" * 80)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Random Forest feature importance\n",
    "rf_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': trained_models['Random_Forest'].feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(f\"\\nRandom Forest - Top 15 Important Features:\")\n",
    "display(rf_importance.head(15))\n",
    "\n",
    "# XGBoost feature importance\n",
    "xgb_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': trained_models['XGBoost'].feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(f\"\\nXGBoost - Top 15 Important Features:\")\n",
    "display(xgb_importance.head(15))\n",
    "\n",
    "# Permutation importance for Random Forest\n",
    "print(\"\\nComputing permutation importance for Random Forest...\")\n",
    "perm_importance_rf = permutation_importance(\n",
    "    trained_models['Random_Forest'], X_test, y_test, \n",
    "    n_repeats=10, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "perm_importance_rf_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance_Mean': perm_importance_rf.importances_mean,\n",
    "    'Importance_Std': perm_importance_rf.importances_std\n",
    "}).sort_values('Importance_Mean', ascending=False)\n",
    "\n",
    "print(\"Random Forest - Top 15 Features (Permutation Importance):\")\n",
    "display(perm_importance_rf_df.head(15))\n",
    "\n",
    "# Visualize feature importance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Random Forest built-in importance\n",
    "top_n = 15\n",
    "rf_top = rf_importance.head(top_n)\n",
    "axes[0].barh(rf_top['Feature'], rf_top['Importance'], color=sns.color_palette(\"viridis\", top_n))\n",
    "axes[0].set_xlabel('Importance Score', fontweight='bold', fontsize=11)\n",
    "axes[0].set_title('Random Forest - Top 15 Features (Built-in Importance)', fontweight='bold', fontsize=12)\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Plot 2: Permutation importance\n",
    "perm_top = perm_importance_rf_df.head(top_n)\n",
    "axes[1].barh(perm_top['Feature'], perm_top['Importance_Mean'], \n",
    "             xerr=perm_top['Importance_Std'],\n",
    "             color=sns.color_palette(\"plasma\", top_n), capsize=5)\n",
    "axes[1].set_xlabel('Permutation Importance', fontweight='bold', fontsize=11)\n",
    "axes[1].set_title('Random Forest - Top 15 Features (Permutation Importance)', fontweight='bold', fontsize=12)\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/09_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Feature importance visualization saved\")\n",
    "\n",
    "# Compare feature importance across models\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Random Forest vs XGBoost importance\n",
    "merged_importance = rf_importance.merge(xgb_importance, on='Feature', suffixes=('_RF', '_XGB'))\n",
    "merged_importance = merged_importance.sort_values('Importance_RF', ascending=False).head(15)\n",
    "\n",
    "x_pos = np.arange(len(merged_importance))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x_pos - width/2, merged_importance['Importance_RF'], width, label='Random Forest', alpha=0.8, color='steelblue')\n",
    "axes[0].bar(x_pos + width/2, merged_importance['Importance_XGB'], width, label='XGBoost', alpha=0.8, color='coral')\n",
    "axes[0].set_xlabel('Feature', fontweight='bold', fontsize=11)\n",
    "axes[0].set_ylabel('Importance', fontweight='bold', fontsize=11)\n",
    "axes[0].set_title('Feature Importance Comparison: RF vs XGBoost', fontweight='bold', fontsize=12)\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels(merged_importance['Feature'], rotation=45, ha='right')\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Scatter plot of feature importance correlation\n",
    "axes[1].scatter(rf_importance['Importance'], xgb_importance['Importance'], alpha=0.6, s=100, color='steelblue', edgecolors='black')\n",
    "axes[1].set_xlabel('Random Forest Importance', fontweight='bold', fontsize=11)\n",
    "axes[1].set_ylabel('XGBoost Importance', fontweight='bold', fontsize=11)\n",
    "axes[1].set_title('Feature Importance Correlation (RF vs XGBoost)', fontweight='bold', fontsize=12)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Add correlation coefficient\n",
    "corr_importance = np.corrcoef(rf_importance['Importance'], xgb_importance['Importance'])[0, 1]\n",
    "axes[1].text(0.95, 0.05, f'Correlation: {corr_importance:.3f}', \n",
    "             transform=axes[1].transAxes, ha='right', va='bottom',\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/10_feature_importance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Feature importance comparison visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34e20e4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 8: SHAP-Based Model Interpretation and Explainability\n",
    "\n",
    "## 8.1: SHAP Analysis Framework\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) values provide theoretically-grounded explanations for individual predictions by computing the contribution of each feature to the model's output. This approach is superior to traditional feature importance because it explains predictions at both the global level (what features matter) and local level (why a specific student was classified as having a particular spatial intelligence level).\n",
    "\n",
    "We'll generate:\n",
    "1. **SHAP Summary Plots** - Global feature importance and direction of influence\n",
    "2. **SHAP Dependency Plots** - How individual feature values affect predictions\n",
    "3. **SHAP Force Plots** - Detailed explanation of individual predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
