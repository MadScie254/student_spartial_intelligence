{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30b0da91",
   "metadata": {},
   "source": [
    "# SpatialIQ: Predicting Students' Spatial Intelligence through Machine Learning\n",
    "\n",
    "## A Comprehensive AI-in-Education Research Study\n",
    "\n",
    "**Author:** AI Research Developer  \n",
    "**Date:** November 2025  \n",
    "**Dataset Citation:** DOI: 10.21227/5qxw-bw66  \n",
    "**Purpose:** Interpretable prediction and analysis of spatial intelligence in high school students using behavioral, academic, and demographic features\n",
    "\n",
    "---\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "This notebook presents a rigorous machine learning pipeline designed to predict students' **Spatial Intelligence** levels (ranging from Very Low to Very High) using a rich dataset of 40 behavioral, academic, and demographic features collected from high school students. Spatial intelligence—the ability to visualize, manipulate, and reason about spatial relationships—is a critical component of cognitive development and academic achievement, particularly in STEM disciplines.\n",
    "\n",
    "### Research Questions:\n",
    "1. **What behavioral and academic patterns predict spatial intelligence?**\n",
    "2. **How do demographic factors (gender, parental education, environment) influence spatial reasoning ability?**\n",
    "3. **Can we build interpretable models that identify actionable insights for educators?**\n",
    "4. **What ethical considerations arise from AI-driven student cognitive profiling?**\n",
    "\n",
    "### Methodology Overview:\n",
    "- **Phase 1:** Comprehensive data exploration and statistical profiling\n",
    "- **Phase 2:** Intelligent feature engineering with domain knowledge\n",
    "- **Phase 3:** Competitive modeling with Logistic Regression, Random Forest, XGBoost, and Neural Networks\n",
    "- **Phase 4:** Deep model interpretation using SHAP explainability methods\n",
    "- **Phase 5:** Actionable insights and ethical considerations\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset Overview\n",
    "\n",
    "The dataset comprises 398 high school students with 40 features across five categories:\n",
    "- **Demographics:** Age, Gender, Class size, Environment (Urban/Suburban/Rural)\n",
    "- **Socioeconomic:** Family size, Parental occupation, Parental education, Income level\n",
    "- **Academic:** Major, GPA, Study time, Extra classes, Teacher assessment\n",
    "- **Behavioral:** Internet usage, TV watching, Pattern recognition, Geographic familiarity\n",
    "- **Gaming Preferences:** Action, Adventure, Strategy, Sport, Simulation, Role-playing, Puzzle games\n",
    "- **Learning Modes:** Visual vs. Auditory, Map usage, Diagram usage, Experience with GIS\n",
    "\n",
    "**Target Variable:** Spatial Intelligence (Categorical: VL, L, M, H, VH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d1d9e5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 1: Import Required Libraries and Dataset Loading\n",
    "\n",
    "This foundational section initializes all necessary libraries for data manipulation, visualization, machine learning, and model interpretation. We leverage industry-standard tools including pandas for data handling, scikit-learn for preprocessing and modeling, XGBoost for gradient boosting, TensorFlow for neural networks, and SHAP for model explainability. This comprehensive toolkit enables both rigorous statistical analysis and production-grade machine learning workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252c3951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== LIBRARY IMPORTS ====================\n",
    "# Core Data Science and Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Data Preprocessing and Encoding\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Model Selection, Training, and Evaluation\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Metrics and Evaluation\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score,\n",
    "    roc_curve, auc, matthews_corrcoef\n",
    ")\n",
    "\n",
    "# Statistical Analysis\n",
    "from scipy.stats import chi2_contingency, mutual_info_classif, spearmanr, pearsonr\n",
    "from scipy.stats import kurtosis, skew\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Feature Importance and SHAP Interpretability\n",
    "import shap\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Warnings and Configuration\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration for visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"✓ All libraries successfully imported\")\n",
    "print(f\"✓ NumPy version: {np.__version__}\")\n",
    "print(f\"✓ Pandas version: {pd.__version__}\")\n",
    "print(f\"✓ Scikit-learn version: {pd.__version__.split('.')[0]}\")\n",
    "print(f\"✓ SHAP library initialized for model interpretation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efe7f52",
   "metadata": {},
   "source": [
    "## 1.1: Load and Inspect the Dataset\n",
    "\n",
    "In this subsection, we load the Dataset.csv file and perform initial exploratory inspection to understand the data structure, dimensions, data types, and preliminary statistical characteristics. This foundational step is critical for identifying potential data quality issues, missing values, and the nature of features we'll be working with throughout the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f676aea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from CSV file\n",
    "data_path = Path(\"data/Dataset.csv\")\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASET LOADING AND INITIAL INSPECTION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n✓ Dataset successfully loaded from: {data_path}\")\n",
    "print(f\"\\nDataset Shape: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "print(f\"Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Display column names and data types\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COLUMN INFORMATION AND DATA TYPES\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nColumn Names and Data Types:\")\n",
    "print(df.info())\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FIRST 10 ROWS OF DATA\")\n",
    "print(\"=\" * 80)\n",
    "display(df.head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "display(df.describe().transpose())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MISSING VALUES ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_counts.index,\n",
    "    'Missing_Count': missing_counts.values,\n",
    "    'Missing_Percentage': missing_percentage.values\n",
    "})\n",
    "missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    print(f\"\\nColumns with missing values ({len(missing_df)} found):\")\n",
    "    display(missing_df)\n",
    "else:\n",
    "    print(\"\\n✓ No missing values detected in the dataset!\")\n",
    "\n",
    "# Identify target variable\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TARGET VARIABLE IDENTIFICATION\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nDataset column names:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")\n",
    "\n",
    "# The last column is typically the target (Spatial Intelligence level)\n",
    "target_col = df.columns[-1]\n",
    "print(f\"\\n✓ Identified target variable: '{target_col}'\")\n",
    "print(f\"Unique values in target: {df[target_col].unique()}\")\n",
    "print(f\"Target value counts:\\n{df[target_col].value_counts().sort_index()}\")\n",
    "\n",
    "# Create a reference dictionary for later use\n",
    "target_mapping = {val: idx for idx, val in enumerate(sorted(df[target_col].unique()))}\n",
    "print(f\"\\nTarget ordinal mapping: {target_mapping}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404e2f51",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 2: Exploratory Data Analysis and Statistical Profiling\n",
    "\n",
    "## 2.1: Distribution Analysis of Spatial Intelligence\n",
    "\n",
    "Spatial Intelligence is our target variable, classified into five ordinal categories ranging from Very Low (VL) to Very High (VH). Understanding the distribution of this outcome variable is crucial for identifying potential class imbalance issues and informing our choice of evaluation metrics and sampling strategies. We'll visualize this distribution through multiple perspectives: raw counts, percentages, and normalized visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc87be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the distribution of the target variable\n",
    "print(\"=\" * 80)\n",
    "print(\"TARGET VARIABLE DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "target_distribution = df[target_col].value_counts().sort_index()\n",
    "target_percentages = (target_distribution / len(df)) * 100\n",
    "\n",
    "distribution_df = pd.DataFrame({\n",
    "    'Spatial_Intelligence_Level': target_distribution.index,\n",
    "    'Count': target_distribution.values,\n",
    "    'Percentage': target_percentages.values,\n",
    "    'Cumulative_Percentage': target_percentages.cumsum().values\n",
    "})\n",
    "\n",
    "print(\"\\nTarget Variable Distribution:\")\n",
    "display(distribution_df)\n",
    "\n",
    "# Calculate class balance metrics\n",
    "class_weights = len(df) / (len(target_distribution) * target_distribution)\n",
    "print(f\"\\nClass Balance Weights (for handling imbalance):\")\n",
    "for level, weight in zip(target_distribution.index, class_weights):\n",
    "    print(f\"  {level}: {weight:.4f}\")\n",
    "\n",
    "# Create visualization of target distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot with counts\n",
    "axes[0].bar(target_distribution.index, target_distribution.values, color=sns.color_palette(\"husl\", len(target_distribution)))\n",
    "axes[0].set_xlabel('Spatial Intelligence Level', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Distribution of Spatial Intelligence (Counts)', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(target_distribution.values):\n",
    "    axes[0].text(i, v + 1, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# Pie chart with percentages\n",
    "colors = sns.color_palette(\"husl\", len(target_distribution))\n",
    "wedges, texts, autotexts = axes[1].pie(\n",
    "    target_distribution.values,\n",
    "    labels=target_distribution.index,\n",
    "    autopct='%1.1f%%',\n",
    "    colors=colors,\n",
    "    startangle=90\n",
    ")\n",
    "axes[1].set_title('Distribution of Spatial Intelligence (Percentage)', fontsize=13, fontweight='bold')\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('white')\n",
    "    autotext.set_fontweight('bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/01_target_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Target distribution visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7594f6bf",
   "metadata": {},
   "source": [
    "## 2.2: Feature Type Classification and Statistical Profiling\n",
    "\n",
    "Before conducting deeper analysis, we need to classify features by type (numeric, categorical ordinal, categorical nominal) and understand their individual distributions. This classification will inform our encoding strategies and feature engineering decisions. We'll compute key statistical properties including skewness, kurtosis, and outlier presence for numeric features, and frequency distributions for categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926189e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs('visualizations', exist_ok=True)\n",
    "\n",
    "# Separate features from target\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "# Classify features by data type\n",
    "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FEATURE TYPE CLASSIFICATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nNumeric Features ({len(numeric_features)} total):\")\n",
    "for feat in numeric_features:\n",
    "    print(f\"  • {feat}\")\n",
    "\n",
    "print(f\"\\nCategorical Features ({len(categorical_features)} total):\")\n",
    "for feat in categorical_features:\n",
    "    print(f\"  • {feat}\")\n",
    "\n",
    "print(f\"\\n✓ Total Features (excluding target): {len(numeric_features) + len(categorical_features)}\")\n",
    "\n",
    "# Statistical profiling of numeric features\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"NUMERIC FEATURES: STATISTICAL PROFILING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "numeric_stats = []\n",
    "for feat in numeric_features:\n",
    "    stats = {\n",
    "        'Feature': feat,\n",
    "        'Mean': X[feat].mean(),\n",
    "        'Std': X[feat].std(),\n",
    "        'Min': X[feat].min(),\n",
    "        'Max': X[feat].max(),\n",
    "        'Median': X[feat].median(),\n",
    "        'Skewness': skew(X[feat].dropna()),\n",
    "        'Kurtosis': kurtosis(X[feat].dropna()),\n",
    "        'Unique_Values': X[feat].nunique()\n",
    "    }\n",
    "    numeric_stats.append(stats)\n",
    "\n",
    "numeric_stats_df = pd.DataFrame(numeric_stats)\n",
    "print(\"\\nNumeric Features Summary:\")\n",
    "display(numeric_stats_df.round(4))\n",
    "\n",
    "# Statistical profiling of categorical features\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CATEGORICAL FEATURES: VALUE DISTRIBUTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "categorical_stats = []\n",
    "for feat in categorical_features:\n",
    "    stats = {\n",
    "        'Feature': feat,\n",
    "        'Data_Type': X[feat].dtype,\n",
    "        'Unique_Values': X[feat].nunique(),\n",
    "        'Top_Value': X[feat].mode()[0] if len(X[feat].mode()) > 0 else 'N/A',\n",
    "        'Top_Value_Freq': X[feat].value_counts().iloc[0] if len(X[feat].value_counts()) > 0 else 0,\n",
    "        'Missing_Values': X[feat].isnull().sum()\n",
    "    }\n",
    "    categorical_stats.append(stats)\n",
    "\n",
    "categorical_stats_df = pd.DataFrame(categorical_stats)\n",
    "print(\"\\nCategorical Features Summary:\")\n",
    "display(categorical_stats_df)\n",
    "\n",
    "# Visualize distributions of numeric features\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VISUALIZING NUMERIC FEATURE DISTRIBUTIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "n_numeric = len(numeric_features)\n",
    "n_cols = 4\n",
    "n_rows = (n_numeric + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 3*n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feat in enumerate(numeric_features):\n",
    "    axes[idx].hist(X[feat].dropna(), bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_title(f'{feat}\\n(Skew: {skew(X[feat].dropna()):.2f})', fontweight='bold')\n",
    "    axes[idx].set_xlabel('Value')\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(n_numeric, len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/02_numeric_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Numeric distributions saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb24ce60",
   "metadata": {},
   "source": [
    "## 2.3: Categorical Feature Distributions and Demographic Insights\n",
    "\n",
    "Understanding the distribution of categorical features provides crucial insights into the composition of our study population. We'll analyze the frequency distributions across key demographic variables including gender, environment, major, and parental occupation. These distributions will help us understand potential biases in our dataset and inform our data preprocessing decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb6335b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis of key categorical features\n",
    "print(\"=\" * 80)\n",
    "print(\"CATEGORICAL FEATURE DETAILED ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Select key demographic features for visualization\n",
    "key_categorical = categorical_features[:8] if len(categorical_features) >= 8 else categorical_features\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feat in enumerate(key_categorical):\n",
    "    value_counts = X[feat].value_counts()\n",
    "    axes[idx].barh(value_counts.index, value_counts.values, color=sns.color_palette(\"husl\", len(value_counts)))\n",
    "    axes[idx].set_title(f'{feat}\\n({len(value_counts)} categories)', fontweight='bold')\n",
    "    axes[idx].set_xlabel('Frequency')\n",
    "    axes[idx].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(value_counts.values):\n",
    "        axes[idx].text(v + 0.5, i, str(v), va='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(len(key_categorical), len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/03_categorical_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Categorical distributions visualization saved\")\n",
    "\n",
    "# Print value counts for each categorical feature\n",
    "for feat in categorical_features:\n",
    "    print(f\"\\n{feat}:\")\n",
    "    print(X[feat].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f1b171",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 3: Data Preprocessing and Intelligent Encoding\n",
    "\n",
    "## 3.1: Strategic Encoding Framework\n",
    "\n",
    "This section implements a sophisticated encoding strategy that respects the underlying structure of different feature types:\n",
    "\n",
    "- **Ordinal Encoding:** Applied to features with inherent order (e.g., Spatial Intelligence levels VL→L→M→H→VH, Education levels)\n",
    "- **One-Hot Encoding:** Applied to nominal categorical features without meaningful order (e.g., gender, environment, major)\n",
    "- **Standardization:** Applied to numeric features to ensure comparable scales for algorithms sensitive to feature magnitude\n",
    "\n",
    "This intelligent approach preserves information while preparing data for machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd8b699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define encoding strategy for features\n",
    "print(\"=\" * 80)\n",
    "print(\"ENCODING STRATEGY DEFINITION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Ordinal features with explicit ordering (knowledge-based)\n",
    "# These are features that have a natural hierarchy\n",
    "ordinal_features_mapping = {\n",
    "    # Education levels (if present) - typically: Unemployment < High School < Bachelor < Master < PhD\n",
    "    # Spatial intelligence related orderings will be handled when found\n",
    "    # Identify by examining unique values\n",
    "}\n",
    "\n",
    "# Binary features (Yes/No, 0/1) - can be left as-is or encoded\n",
    "binary_features = []\n",
    "\n",
    "# Examine categorical features to identify ordinal vs nominal\n",
    "print(\"\\nExamining categorical features for ordinal structure:\")\n",
    "for feat in categorical_features:\n",
    "    unique_vals = sorted(X[feat].unique())\n",
    "    print(f\"  {feat}: {unique_vals}\")\n",
    "    \n",
    "    # Check if it looks ordinal based on values\n",
    "    if all(isinstance(v, (int, float)) for v in unique_vals):\n",
    "        binary_features.append(feat)\n",
    "\n",
    "print(f\"\\n✓ Identified {len(binary_features)} binary/numeric-like categorical features\")\n",
    "\n",
    "# Prepare data copy for preprocessing\n",
    "X_processed = X.copy()\n",
    "y_processed = y.copy()\n",
    "\n",
    "# Handle binary/numeric categorical features - convert to numeric if already numeric\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ENCODING IMPLEMENTATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for feat in binary_features:\n",
    "    X_processed[feat] = pd.to_numeric(X_processed[feat], errors='coerce').fillna(X_processed[feat].mode()[0])\n",
    "\n",
    "# Encode target variable using OrdinalEncoder (preserving order VL < L < M < H < VH)\n",
    "target_order = sorted(y_processed.unique())\n",
    "print(f\"\\nTarget variable ordinal encoding: {target_order}\")\n",
    "\n",
    "le_target = LabelEncoder()\n",
    "y_processed = pd.Series(le_target.fit_transform(y_processed), index=y_processed.index)\n",
    "\n",
    "# Separate remaining categorical features for One-Hot Encoding\n",
    "nominal_categorical = [f for f in categorical_features if f not in binary_features]\n",
    "\n",
    "print(f\"\\nFeatures to be One-Hot Encoded ({len(nominal_categorical)} total):\")\n",
    "for feat in nominal_categorical:\n",
    "    print(f\"  • {feat}\")\n",
    "\n",
    "# Apply One-Hot Encoding to nominal categorical features\n",
    "X_encoded = pd.get_dummies(X_processed, columns=nominal_categorical, drop_first=False, dtype=int)\n",
    "\n",
    "print(f\"\\n✓ Encoding complete!\")\n",
    "print(f\"Original features: {X_processed.shape[1]}\")\n",
    "print(f\"Features after encoding: {X_encoded.shape[1]}\")\n",
    "print(f\"New feature columns added: {X_encoded.shape[1] - X_processed.shape[1]}\")\n",
    "\n",
    "# Display encoded feature names\n",
    "print(\"\\nEncoded features:\")\n",
    "encoded_cols = X_encoded.columns.tolist()\n",
    "for i, col in enumerate(encoded_cols, 1):\n",
    "    print(f\"{i:3d}. {col}\")\n",
    "\n",
    "# Standardize numeric features\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"NUMERIC FEATURE STANDARDIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "numeric_features_encoded = X_encoded.select_dtypes(include=[np.number]).columns.tolist()\n",
    "scaler = StandardScaler()\n",
    "X_scaled = X_encoded.copy()\n",
    "X_scaled[numeric_features_encoded] = scaler.fit_transform(X_encoded[numeric_features_encoded])\n",
    "\n",
    "print(f\"\\n✓ Standardized {len(numeric_features_encoded)} numeric features\")\n",
    "print(f\"  Using StandardScaler (mean=0, std=1)\")\n",
    "\n",
    "# Display summary of preprocessing\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PREPROCESSING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nOriginal dataset shape: {df.shape}\")\n",
    "print(f\"Processed dataset shape: {X_scaled.shape}\")\n",
    "print(f\"Target variable shape: {y_processed.shape}\")\n",
    "print(f\"\\n✓ Data preprocessing completed successfully!\")\n",
    "\n",
    "# Show sample of processed data\n",
    "print(\"\\nSample of processed data (first 5 rows):\")\n",
    "display(X_scaled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673b816f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 4: Feature Engineering and Dimensionality Reduction\n",
    "\n",
    "## 4.1: Domain-Driven Feature Engineering\n",
    "\n",
    "In this section, we create scientifically-motivated derived features that capture meaningful combinations of existing variables. Domain expertise in education psychology and cognitive science informs our feature engineering decisions. These engineered features aim to represent latent concepts that directly influence spatial intelligence development.\n",
    "\n",
    "### Engineered Features:\n",
    "1. **Study Efficiency:** Ratio of study time to academic performance (GPA)\n",
    "2. **Academic Support Index:** Combination of extra classes and parental education level\n",
    "3. **Gaming Engagement:** Aggregated preference across multiple game genres\n",
    "4. **Visual Learning Orientation:** Combined score of map and diagram usage\n",
    "5. **Digital Lifestyle:** Combined internet and gaming engagement\n",
    "6. **Family Education:** Average of parental education levels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
